# ü§ñ Installation et Configuration d'Ollama pour HealthVoice

## üìã Qu'est-ce qu'Ollama ?

Ollama est un outil qui permet d'ex√©cuter des mod√®les d'IA (LLM) **localement** sur votre ordinateur, **gratuitement** et **sans connexion internet**.

## üéØ Fonctionnalit√© Int√©gr√©e

Quand vous cr√©ez une nouvelle **Observation**, le syst√®me va **automatiquement** :
1. ‚úÖ Analyser la transcription vocale
2. ‚úÖ Identifier la cat√©gorie m√©dicale (Respiratoire, Cardiovasculaire, etc.)
3. ‚úÖ G√©n√©rer un r√©sum√© des sympt√¥mes
4. ‚úÖ Proposer des recommandations m√©dicales
5. ‚úÖ Cr√©er automatiquement une nouvelle **Analyse** compl√®te

## üì• Installation d'Ollama

### Windows

1. **T√©l√©charger Ollama**
   - Allez sur : https://ollama.ai/download
   - Cliquez sur "Download for Windows"
   - T√©l√©chargez `OllamaSetup.exe`

2. **Installer**
   - Double-cliquez sur `OllamaSetup.exe`
   - Suivez l'assistant d'installation
   - Ollama se lancera automatiquement apr√®s l'installation

3. **V√©rifier l'installation**
   - Ouvrez PowerShell
   - Tapez : `ollama --version`
   - Vous devriez voir la version install√©e

### macOS

```bash
# T√©l√©charger et installer
curl https://ollama.ai/install.sh | sh
```

### Linux

```bash
# Installation
curl https://ollama.ai/install.sh | sh
```

## üß† T√©l√©charger un Mod√®le d'IA

Une fois Ollama install√©, vous devez t√©l√©charger un mod√®le d'IA. Voici les options recommand√©es :

### Option 1 : GPT-OSS 120B Cloud (Recommand√© - Puissant)

```bash
ollama pull gpt-oss:120b-cloud
```

**Taille :** Variable (mod√®le cloud)  
**Performance :** Excellent pour l'analyse m√©dicale  
**Vitesse :** Rapide

### Option 2 : Mistral (Alternative)

```bash
ollama pull mistral
```

**Taille :** ~4 GB  
**Performance :** Tr√®s bon  
**Vitesse :** Moyen

### Option 3 : Gemma (L√©ger)

```bash
ollama pull gemma:2b
```

**Taille :** ~1.5 GB  
**Performance :** Bon  
**Vitesse :** Tr√®s rapide

## ‚öôÔ∏è Configuration du Projet

### 1. V√©rifier le fichier `apiOmk.html`

Le code est d√©j√† int√©gr√© ! V√©rifiez ces lignes (vers la ligne 870) :

```javascript
// Configuration Ollama
const OLLAMA_API_URL = 'http://localhost:11434/api/generate';
const OLLAMA_MODEL = 'gpt-oss:120b-cloud'; // Changez si vous utilisez un autre mod√®le
```

### 2. Changer le Mod√®le (si n√©cessaire)

Si vous avez t√©l√©charg√© `mistral`, `gemma` ou `llama3.2` au lieu de `gpt-oss:120b-cloud`, modifiez cette ligne :

```javascript
const OLLAMA_MODEL = 'mistral'; // ou 'gemma:2b'
```

## üöÄ D√©marrage

### 1. Lancer Ollama

#### Windows
- Ollama se lance automatiquement au d√©marrage
- V√©rifiez dans la barre des t√¢ches (ic√¥ne en bas √† droite)
- Ou lancez manuellement : tapez `ollama serve` dans PowerShell

#### macOS/Linux
```bash
ollama serve
```

### 2. V√©rifier qu'Ollama fonctionne

Ouvrez votre navigateur et allez sur :
```
http://localhost:11434/api/tags
```

Vous devriez voir la liste de vos mod√®les install√©s.

### 3. Lancer HealthVoice

1. D√©marrez votre serveur local (XAMPP, WAMP, etc.)
2. Ouvrez `apiOmk.html` dans votre navigateur
3. Dans la console du navigateur (F12), vous devriez voir :
   ```
   ‚úì Ollama est disponible
   ```

## üé¨ Utilisation

### Cr√©er une Observation avec Analyse Automatique

1. **Allez dans l'onglet "Observations"**
2. **Cliquez sur "Nouvelle Observation"**
3. **Remplissez les champs :**
   - Date
   - Sympt√¥me (ex: Toux, Fi√®vre)
   - Intensit√© (ex: Forte)
   - Transcription vocale : D√©crivez les sympt√¥mes en d√©tail
     ```
     Exemple : "Le patient pr√©sente une toux s√®che persistante depuis 3 jours,
     avec des difficult√©s respiratoires l√©g√®res. Pas de fi√®vre."
     ```
   - Utilisateur

4. **Cliquez sur "Enregistrer"**

5. **Le syst√®me va automatiquement :**
   - ‚úÖ Cr√©er l'observation
   - ‚úÖ Envoyer la transcription √† Ollama
   - ‚úÖ G√©n√©rer une analyse IA
   - ‚úÖ Cr√©er automatiquement une nouvelle "Analyse" avec :
     - Cat√©gorie : (ex: Respiratoire)
     - R√©sum√© : (g√©n√©r√© par l'IA)
     - Recommandations : (g√©n√©r√©es par l'IA)
     - Observation associ√©e : (lien automatique)

6. **V√©rifiez dans l'onglet "Analyses"**
   - Vous verrez la nouvelle analyse cr√©√©e automatiquement
   - Elle sera pr√©fix√©e "Analyse Automatique - [Cat√©gorie]"

## üîç D√©pannage

### ‚ùå "Ollama non disponible"

**Solution 1 :** V√©rifier qu'Ollama est lanc√©
```bash
# Dans PowerShell/Terminal
ollama serve
```

**Solution 2 :** V√©rifier que le port 11434 est disponible
```bash
# Windows PowerShell
netstat -ano | findstr :11434

# macOS/Linux
lsof -i :11434
```

**Solution 3 :** Red√©marrer Ollama
```bash
# Arr√™ter
taskkill /F /IM ollama.exe  # Windows
killall ollama              # macOS/Linux

# Relancer
ollama serve
```

### ‚ö†Ô∏è "Pas de JSON trouv√© dans la r√©ponse"

**Solution :** Le mod√®le n'a pas g√©n√©r√© un JSON valide
- Essayez un autre mod√®le (mistral est parfois plus pr√©cis)
- V√©rifiez que le mod√®le est bien t√©l√©charg√© :
  ```bash
  ollama list
  ```

### üêå "G√©n√©ration tr√®s lente"

**Solutions :**
1. Utilisez un mod√®le plus l√©ger : `gemma:2b`
2. V√©rifiez les ressources de votre PC (RAM, CPU)
3. Ollama utilise le GPU si disponible (plus rapide)

### üö´ "Erreur API Ollama: 404"

**Solution :** Le mod√®le n'existe pas
```bash
# Lister les mod√®les install√©s
ollama list

# T√©l√©charger le mod√®le manquant
ollama pull llama3.2
```

## üìä Tests

### Test Rapide

1. Ouvrez PowerShell/Terminal
2. Testez Ollama directement :

```bash
ollama run llama3.2 "Analyse ces sympt√¥mes: toux, fi√®vre 38¬∞C, fatigue"
```

Vous devriez obtenir une r√©ponse m√©dicale.

### Test dans HealthVoice

1. Ouvrez `apiOmk.html`
2. Ouvrez la console (F12)
3. Tapez :

```javascript
checkOllamaAvailability().then(result => {
    console.log('Ollama disponible:', result);
});
```

Vous devriez voir : `Ollama disponible: true`

## üé® Personnalisation

### Modifier le Prompt de l'IA

Dans `apiOmk.html`, cherchez la fonction `generateAnalysisWithOllama` et modifiez le prompt :

```javascript
const prompt = `Tu es un assistant m√©dical expert...`;
```

Vous pouvez :
- Changer le ton (plus ou moins formel)
- Ajouter des instructions sp√©cifiques
- Demander plus ou moins de d√©tails

### Changer les Cat√©gories

Modifiez la liste des cat√©gories valides :

```javascript
const validCategories = ['Respiratoire', 'Cardiovasculaire', ...];
```

## üìà Performance

### Configuration Minimale
- **RAM :** 8 GB
- **Disque :** 5 GB d'espace libre
- **CPU :** Intel i5 ou √©quivalent

### Configuration Recommand√©e
- **RAM :** 16 GB
- **Disque :** 10 GB d'espace libre (SSD recommand√©)
- **CPU :** Intel i7 ou √©quivalent
- **GPU :** NVIDIA avec CUDA (optionnel mais acc√©l√®re consid√©rablement)

### Temps de G√©n√©ration Estim√©s

| Mod√®le | Configuration Min | Configuration Recommand√©e |
|--------|-------------------|---------------------------|
| gemma:2b | 15-30 sec | 5-10 sec |
| llama3.2 | 30-60 sec | 10-20 sec |
| mistral | 45-90 sec | 15-30 sec |

## üÜò Support

### Ollama
- Site : https://ollama.ai
- Documentation : https://github.com/ollama/ollama
- Discord : https://discord.gg/ollama

### HealthVoice
- Consultez les logs dans la console du navigateur (F12)
- V√©rifiez les messages d'erreur affich√©s √† l'√©cran

## üìù Notes Importantes

‚ö†Ô∏è **Avertissement M√©dical :**
- Les analyses g√©n√©r√©es par l'IA sont √† titre **informatif uniquement**
- Elles **NE REMPLACENT PAS** un avis m√©dical professionnel
- Toujours consulter un m√©decin pour un diagnostic

‚úÖ **Avantages :**
- 100% gratuit
- Fonctionne hors ligne
- Donn√©es priv√©es (tout reste sur votre ordinateur)
- Aucune limite d'utilisation

üîí **S√©curit√© :**
- Toutes les donn√©es restent locales
- Aucune donn√©e envoy√©e sur internet
- Conforme au RGPD

## üéâ C'est tout !

Une fois Ollama install√© et configur√©, vos observations g√©n√©reront automatiquement des analyses IA ! üöÄ
